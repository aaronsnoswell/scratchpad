{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Weight Initialization\n",
    "\n",
    "Consider a Multi Layer Perceptron (MLP) Neural Network where one layer consists of the function $y_i(x) = \\sigma(W_i x + b_i)$, where $x$ is the input $n$-vector, $W_i$ is an $m \\times n$ weight matrix, $b$ is a bias $m$-vector and $\\sigma : \\mathbb{R}^m \\to \\mathbb{R}^m$ is some elementwise activation function.\n",
    "We can then represent the entire network as repeated application of the layer function;\n",
    "\n",
    "$$\n",
    "o(x) = (y_N \\circ y_{N-1} \\circ \\dots \\circ y_2 \\circ y_1)(x)\n",
    "$$\n",
    "\n",
    "Assume the input values are drawn from a standard normal distribution $x \\sim \\mathcal{N}(0, 1)$ and bias vectors are initialized as zero vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity activation function\n",
    "\n",
    "With an identity activation function $\\sigma(x) = x$, the appropriate initialization for weight matrices is $W_i \\sim \\mathcal{N}(0, \\frac{1}{n_i})$, where $n_i$ is the size of the $i$th-layer input vector.\n",
    "This can be achieved by adjusting the weight initialization distribution, or by using a standard normal, then dividing the weights by $\\sqrt{n_i}$.\n",
    "\n",
    "This is sometimes called 'Xavier' or 'Glorot' intialization, named after Xavier Glorot from the 2010 paper <a href=\"http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi\">Understanding the difficulty of training deep feedforward neural networks</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get imports\n",
    "import math\n",
    "import random\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0088) tensor(0.9887)\n",
      "tensor(-0.0213) tensor(1.6131)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use identity activation function\n",
    "sigma = lambda x: x\n",
    "\n",
    "# Initialize a random deep network\n",
    "num_layers = 200\n",
    "random_layer_size = lambda: random.randint(10, 1024)\n",
    "x = torch.randn(random_layer_size())\n",
    "weights = []\n",
    "biases = []\n",
    "layer_input_size = len(x)\n",
    "for layer in range(num_layers):\n",
    "    layer_output_size = random_layer_size()\n",
    "    weights.append(\n",
    "        torch.randn(\n",
    "            layer_output_size,\n",
    "            layer_input_size\n",
    "        ) / math.sqrt(layer_input_size)\n",
    "    )\n",
    "    biases.append(torch.zeros(layer_output_size))\n",
    "    layer_input_size = layer_output_size\n",
    "\n",
    "# Forward pass through the network\n",
    "y = x\n",
    "for w, b in zip(weights, biases):\n",
    "    y = sigma(w @ y + b)\n",
    "\n",
    "# Check the distribution of the output vector\n",
    "print(x.mean(), x.std())\n",
    "print(y.mean(), y.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Without the normalization on line 18 above, the values in the network will tend toward 0 or a very large value, slowing training.\n",
    "With Xavier initialization, the layer values maintain a normal distribution, regardless of the depth of the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ReLU activation function\n",
    "\n",
    "With the Rectified Linear Unit (ReLU) activation function $\\sigma(x) = \\max(0, x)$, the appropriate initialization for weight matrices is $W_i \\sim \\mathcal{N}(0, \\frac{2}{n_i})$, where $n_i$ is the size of the $i$th-layer input vector.\n",
    "\n",
    "This is sometimes called 'Kaiming' or 'He' intialization, named after Kaiming He from the 2015 paper <a href=\"https://arxiv.org/pdf/1502.01852.pdf\">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0279) tensor(0.9784)\n",
      "tensor(0.0316) tensor(0.0482)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use ReLU activation function\n",
    "sigma = lambda x: torch.max(torch.zeros_like(x), x)\n",
    "\n",
    "# Initialize a random deep network\n",
    "num_layers = 200\n",
    "random_layer_size = lambda: random.randint(10, 1024)\n",
    "x = torch.randn(random_layer_size())\n",
    "weights = []\n",
    "biases = []\n",
    "layer_input_size = len(x)\n",
    "for layer in range(num_layers):\n",
    "    layer_output_size = random_layer_size()\n",
    "    weights.append(\n",
    "        torch.randn(\n",
    "            layer_output_size,\n",
    "            layer_input_size\n",
    "        ) * math.sqrt(2) / math.sqrt(layer_input_size)\n",
    "    )\n",
    "    biases.append(torch.zeros(layer_output_size))\n",
    "    layer_input_size = layer_output_size\n",
    "\n",
    "# Forward pass through the network\n",
    "y = x\n",
    "for w, b in zip(weights, biases):\n",
    "    y = sigma(w @ y + b)\n",
    "\n",
    "# Check the distribution of the output vector\n",
    "print(x.mean(), x.std())\n",
    "print(y.mean(), y.std())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
